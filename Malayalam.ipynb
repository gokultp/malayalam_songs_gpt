{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gokultp/malayalam_songs_gpt/blob/master/Malayalam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIRo6DBJUwU5",
        "outputId": "8e0f1c93-42ea-46d1-9ed3-4c3aa26f58fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-10 08:11:10--  https://raw.githubusercontent.com/gokultp/malayalam_songs_gpt/master/data/songs.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34085911 (33M) [text/plain]\n",
            "Saving to: ‘songs.txt’\n",
            "\n",
            "songs.txt           100%[===================>]  32.51M   194MB/s    in 0.2s    \n",
            "\n",
            "2024-06-10 08:11:11 (194 MB/s) - ‘songs.txt’ saved [34085911/34085911]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/gokultp/malayalam_songs_gpt/master/data/songs.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L11cDp-RPAAe"
      },
      "source": [
        "Install and setup sentence piece tokeniser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dwE31bR1G_gj"
      },
      "outputs": [],
      "source": [
        "with open('songs.txt') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[101:148]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zNPsRNo0NFne",
        "outputId": "8d3ab6af-6c6c-4054-eda7-bddf067e1936"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' ഹരിഗോവിന്ദാ..ഗോവിന്ദാ ഹരിഗോവിന്ദാ..(4)\\nസമാനഹൃദ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LVgKGTRPAoD",
        "outputId": "c9ad004e-6945-4a73-a7d6-9694c88143ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZMNLf4P1OHHh"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeB0s-UiOJZE",
        "outputId": "4ff2e977-fe09-417d-ca85-a88e63fd7007"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7994d254dd50>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# hyperparameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 48 # what is the maximum context length for predictions?\n",
        "max_iters = 20000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a little optimiation here. We are training with only malayalam characters. The tiktoken has nearly 100k vocabs and all are not usefull for us\n",
        "\n",
        "Our vocab is much smaller. So writing a custom encoder to reduce the complexity"
      ],
      "metadata": {
        "id": "bWhpc23TQRts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uND39UYkGp2Y"
      },
      "outputs": [],
      "source": [
        "uniq_vocabs = {num: token for num, token in enumerate(list(set(encoder.encode(text)))) }\n",
        "\n",
        "tok_to_id = {token: num for num, token in uniq_vocabs.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uniq_vocabs"
      ],
      "metadata": {
        "id": "UB2mMCPNcjvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda txt : [tok_to_id[y] for y in encoder.encode(txt)]\n",
        "decode = lambda id_lst: encoder.decode([uniq_vocabs[y] for y in id_lst])\n",
        "\n"
      ],
      "metadata": {
        "id": "lCa5DVOgOEMx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test if both encoder and decoder works as expected"
      ],
      "metadata": {
        "id": "O-yEzjSYQusl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decode(encode(text[:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_cEIAtw-PbCR",
        "outputId": "6753e12b-d809-4608-8d42-c0376574b259"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'മീരാ..മീരാ..മീരാ മീരാ..\\nകൃഷ്ണ കനയ്യാ,,കൃഷ്ണ കനയ്യാ..\\nകൃഷ്ണ കനയ്യ കൃഷ്ണ ഹരേ..\\nമീരാ...മീരാ....\\nഗോവിന്ദ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(uniq_vocabs)\n",
        "print( f\"reduced vocabs to much smaller number : {vocab_size} from {encoder.n_vocab}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5NAYmiDQ9G6",
        "outputId": "9e2050ec-0fef-4aeb-b7cf-18ca424a1498"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reduced vocabs to much smaller number : 355 from 100277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text))"
      ],
      "metadata": {
        "id": "rM3de0_eR3WJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "nELeAvAlSIkf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "G6aZfnisYSdQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(decode(xb[0].tolist()))\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(decode(yb[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xARjMA1YVim",
        "outputId": "5f7e594c-147f-44af-b4e2-a9bf5333751e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "�ഭഗാ�\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "ഭഗാന\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class MLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "ujLJtTg-Yx1A"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = MLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "YUi3SDHPY-fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "d_qon2AEL0q8"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb2-mnBTL6I8",
        "outputId": "6e8f19cc-9d01-4803-9753-a71bedbba3ca"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7748582363128662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        ""
      ],
      "metadata": {
        "id": "BCxO29l8MT2c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzOgameqNuyW",
        "outputId": "00f042c3-857a-4033-82bb-df27347570f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.248163 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OM_WUy8OL2f",
        "outputId": "f87c6e46-a991-4c3e-c913-e3c9659682ea"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 1.2048, val loss 1.2070\n",
            "step 300: train loss 1.2000, val loss 1.1982\n",
            "step 600: train loss 1.1963, val loss 1.1959\n",
            "step 900: train loss 1.1972, val loss 1.1948\n",
            "step 1200: train loss 1.1880, val loss 1.1954\n",
            "step 1500: train loss 1.1903, val loss 1.1823\n",
            "step 1800: train loss 1.1832, val loss 1.1862\n",
            "step 2100: train loss 1.1752, val loss 1.1836\n",
            "step 2400: train loss 1.1790, val loss 1.1829\n",
            "step 2700: train loss 1.1729, val loss 1.1768\n",
            "step 3000: train loss 1.1721, val loss 1.1726\n",
            "step 3300: train loss 1.1726, val loss 1.1704\n",
            "step 3600: train loss 1.1744, val loss 1.1758\n",
            "step 3900: train loss 1.1624, val loss 1.1737\n",
            "step 4200: train loss 1.1718, val loss 1.1757\n",
            "step 4500: train loss 1.1685, val loss 1.1670\n",
            "step 4800: train loss 1.1647, val loss 1.1734\n",
            "step 5100: train loss 1.1621, val loss 1.1619\n",
            "step 5400: train loss 1.1594, val loss 1.1607\n",
            "step 5700: train loss 1.1573, val loss 1.1640\n",
            "step 6000: train loss 1.1501, val loss 1.1511\n",
            "step 6300: train loss 1.1532, val loss 1.1578\n",
            "step 6600: train loss 1.1548, val loss 1.1545\n",
            "step 6900: train loss 1.1481, val loss 1.1594\n",
            "step 7200: train loss 1.1489, val loss 1.1524\n",
            "step 7500: train loss 1.1504, val loss 1.1519\n",
            "step 7800: train loss 1.1503, val loss 1.1526\n",
            "step 8100: train loss 1.1501, val loss 1.1514\n",
            "step 8400: train loss 1.1430, val loss 1.1469\n",
            "step 8700: train loss 1.1390, val loss 1.1475\n",
            "step 9000: train loss 1.1520, val loss 1.1426\n",
            "step 9300: train loss 1.1435, val loss 1.1512\n",
            "step 9600: train loss 1.1447, val loss 1.1467\n",
            "step 9900: train loss 1.1425, val loss 1.1396\n",
            "step 10200: train loss 1.1394, val loss 1.1400\n",
            "step 10500: train loss 1.1367, val loss 1.1451\n",
            "step 10800: train loss 1.1312, val loss 1.1403\n",
            "step 11100: train loss 1.1341, val loss 1.1421\n",
            "step 11400: train loss 1.1311, val loss 1.1395\n",
            "step 11700: train loss 1.1372, val loss 1.1371\n",
            "step 12000: train loss 1.1329, val loss 1.1339\n",
            "step 12300: train loss 1.1314, val loss 1.1396\n",
            "step 12600: train loss 1.1281, val loss 1.1318\n",
            "step 12900: train loss 1.1272, val loss 1.1309\n",
            "step 13200: train loss 1.1283, val loss 1.1315\n",
            "step 13500: train loss 1.1299, val loss 1.1395\n",
            "step 13800: train loss 1.1272, val loss 1.1276\n",
            "step 14100: train loss 1.1265, val loss 1.1289\n",
            "step 14400: train loss 1.1236, val loss 1.1317\n",
            "step 14700: train loss 1.1225, val loss 1.1360\n",
            "step 15000: train loss 1.1257, val loss 1.1358\n",
            "step 15300: train loss 1.1157, val loss 1.1261\n",
            "step 15600: train loss 1.1204, val loss 1.1237\n",
            "step 15900: train loss 1.1190, val loss 1.1240\n",
            "step 16200: train loss 1.1224, val loss 1.1279\n",
            "step 16500: train loss 1.1225, val loss 1.1194\n",
            "step 16800: train loss 1.1178, val loss 1.1294\n",
            "step 17100: train loss 1.1203, val loss 1.1214\n",
            "step 17400: train loss 1.1194, val loss 1.1261\n",
            "step 17700: train loss 1.1106, val loss 1.1235\n",
            "step 18000: train loss 1.1121, val loss 1.1179\n",
            "step 18300: train loss 1.1189, val loss 1.1241\n",
            "step 18600: train loss 1.1155, val loss 1.1221\n",
            "step 18900: train loss 1.1140, val loss 1.1207\n",
            "step 19200: train loss 1.1126, val loss 1.1183\n",
            "step 19500: train loss 1.1087, val loss 1.1194\n",
            "step 19800: train loss 1.1158, val loss 1.1204\n",
            "step 19999: train loss 1.1167, val loss 1.1176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.full((1, 1), 87, dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnlPKW40P0UR",
        "outputId": "a0b2ee34-bee8-4198-c240-027c91e1084c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "വാപന മാറിലെ\n",
            "കനകക്കുന്നുപോരാടെ മദനദളചനയായി\n",
            "നെറു പെറിയാമെന്നും ചുടക്കടയാലേഖ\n",
            "വിടരൂ തുള്ളാൻ പെണ്ണീരിന്റെ സാഗരതീ ബിരേതം\n",
            "നക്ഷത്രിപ്ദാക്കും ച൮യുന്നു നാട്ടുന്ന നഹൃദയമലരണിതളയടില്‍ മൂനയിനേഴോ\n",
            "ജനന്മങ്ങള്‍ വാഴണ്ണി നിറപറകുകള്‍\n",
            "നിരുകിവട കഴിയാളിന്നുണ്ടേ\n",
            "മോഹംമെയ്യിൽ മൊയ്യാങ്കുല പൂങ്കൂടരാമോ\n",
            "മുന്മിയതു നിലാവികാരങ്ങളാകെ, മതിമാതനാണിനി കാതില്‍\n",
            "മദനസിയുന്നരിയില്‍ പോവാവില്‍\n",
            "അരിയതോം അനുരാഗ്ളി അഴര്‍പൂങ്കാറ്റേ....\n",
            "\n",
            "പ്രാരേ എടാന്‍\n",
            "കുണിചിരടി മനസ്സിക്കീ ഗാനം\n",
            "ഒരേ ഇതിലേ തിത്തിക നനയേ\n",
            "മ..മഹി�്രീ...\n",
            "അകമെത്തമേ പൂത്തു വിടരുന്നു\n",
            "ആ )\n",
            "കതതകളിലെ പഴകൾ യൂടലെല്ലേ...\n",
            "നാം വീഴും കാത്തിയ കൊണ്ടു\n",
            "നാണമാണോട്ടേ\n",
            "മടയടിയേ പരവും  കലമേ കറങ്ങൂ...\n",
            "കുനനേനോ നീ\n",
            "നമ്മിതാ ചാരത്തെ\n",
            "കടലേ താളങ്ങൾ അഴകിലെഴും....\n",
            "ഇണമേണക്കുപാടി കരളിനെ കൊറ്റപ്പിഞ്ചസ്സാരേ\n",
            "എന്നു മഴ പടിയല്ലൊരു മാടം മുങ്ങേല...\n",
            "നിലവാക്കിളിയായൊതുപ്പോയ് കുളിതിരകളിലല്‍ താനേ\n",
            "കാട്ടിനും കാറ്റേ ‍\n",
            "ഹം...നീയ നൽക്കുമോ ...\n",
            "വിരഭത്തി മയക്കാരേ...\n",
            "കാറ്റേ കാറ്റേ താംബിക്കത്തിരിക്കു നല്‍കങ്ങളില്‍ (2)\n",
            "ആണിക്കാറ്റും നാം കൊഴിയും താരം\n",
            "പ്രണവും ദിവടടിയാലെന്തിച്ചവന്‍\n",
            "ഗായൂജീരന്‍ രാഗജാനം...)\n",
            "ഓലമായിചാര്‍ത്തിടാം എന്നും നീയും\n",
            "നീലിത്തിത്തളിഞ്ഞുനോ കൂടോലത്തെ\n",
            "പാദസിരലിൽ നിൻ മധുരാൻ കൃഷ്ണാ\n",
            "[ഇരുൾ തിരയേവൾ...)\n",
            "ആരെടി\n",
            "(ഇളംപോലെ...)\n",
            "കൃപ്രണയം കൂണ്ടില്ലല്ലയോ കാട്ടുണ്ണാൻ എന്ദമാം\n",
            "പഞ്ചാരം ആവടേ മാലകത്തിൽ\n",
            "തൂങ്കൊരെട്ടിക്കാനിന്ദേപലേ\n",
            "ഇലനെയൊരുത്തിയിൽ സംഗീതവാൻ\n",
            "ഓ പോ�\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "__7eotBZVPqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encode('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAPFJOxaQm93",
        "outputId": "bd10920e-7afa-46f2-ac15-6caf81d2e7d7"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[87]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGFgFgiV7WyAxqnZxD+G6q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}